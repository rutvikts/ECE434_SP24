{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPszpz29ddUD"
   },
   "source": [
    "# ECE/CS 434 | MP2: DUET\n",
    "<br />\n",
    "<nav>\n",
    "    <span class=\"alert alert-block alert-warning\">Due on Wednesday Feb 21 11:59PM on Gradescope</span>\n",
    "   \n",
    "</nav><br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t9qXUJkddUH"
   },
   "source": [
    "## Objective\n",
    "In this MP, you will:\n",
    "- Implement DUET algorithm to separate a mixture of N voice signals from received from two microphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g3IdFpLddUH"
   },
   "source": [
    "---\n",
    "## Problem Overview\n",
    "Consider a problem of separating N sources ($S_1$, $S_2$, ... $S_N$) from recordings on 2 microphones ($R_1$ and $R_2$).\n",
    "According to DUET algorithm, you will need to perform the following steps:\n",
    "\n",
    "- Calculate the short-time Fourier transform of two received signals to get the time-frequency spectrograms\n",
    "- Calculate the ratio of the two time-frequency spectrograms to get relative delay and attenuation\n",
    "- Cluster the time-frequency bins in the 2D space spanned by relative delay and attenuation\n",
    "- Recover the original N signals based on the clustering results\n",
    "\n",
    "You can refer to the original DUET paper in ICASSP 2000: \"Blind separation of disjoint orthogonal signals: demixing N sources from 2 mixtures\" and this tutorial in Blind speech separation, 2007 - Springer: \"The DUET blind source separation algorithm\"\n",
    "\n",
    "For the sake of easier clustering, the exact number of sources N will be provided to you.\n",
    "\n",
    "You can assume there is no time-frequency bin collision for any two sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nC62qJrddUI"
   },
   "source": [
    "---\n",
    "## Imports & Setup\n",
    "To run the grading script of this MP, you will need to install the Python [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) package. The SpeechRecognition package also requires the dependency [pocketsphinx](https://pypi.org/project/pocketsphinx/). You may directly use pip install to install both packages.\n",
    "The following `code` cell, when run, imports the libraries you might need for this MP. Feel free to delete or import other commonly used libraries. Double check with the TA if you are unsure if a library is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uh9Jqy_YddUI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Custom Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import cluster\n",
    "\n",
    "# for np.ceil\n",
    "import math\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    # plt.style.use(\"seaborn\") # This sets the matplotlib color scheme to something more soothing\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# This function is used to format test results. You don't need to touch it.\n",
    "def display_table(data):\n",
    "    from IPython.display import HTML, display\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td><h4>%s</h4><td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljqhWoW4ddUJ"
   },
   "source": [
    "### Sanity-check\n",
    "\n",
    "Running the following code block verifies that the correct module versions are indeed being used. \n",
    "\n",
    "Try restarting the Python kernel (or Jupyter) if there is a mismatch even after intalling the correct version. This might happen because Python's `import` statement does not reload already-loaded modules even if they are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dixvUHQ_ddUK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style='color:#4caf50;weight:700;'>[âœ“] scipy version 1.7.2 is correct.</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style='color:#4caf50;weight:700;'>[âœ“] speech_recognition version 3.9.0 is correct.</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style='color:#4caf50;weight:700;'>[âœ“] numpy version 1.21.3 is correct.</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style='color:#4caf50;weight:700;'>[âœ“] matplotlib version 3.4.3 is correct.</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<text style='color:#4caf50;weight:700;'>[âœ“] scipy version 1.7.2 is correct.</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    def printc(text, color):\n",
    "        display(HTML(\"<text style='color:{};weight:700;'>{}</text>\".format(color, text)))\n",
    "\n",
    "    _requirements = [r.split(\"==\") for r in open(\n",
    "        \"packages.txt\", \"r\").read().split(\"\\n\")]\n",
    "\n",
    "    import sys\n",
    "    for (module, expected_version) in _requirements:\n",
    "        try:\n",
    "            if sys.modules[module].__version__ != expected_version:\n",
    "                printc(\"[âœ•] {} version should to be {}, but {} is installed.\".format(\n",
    "                    module, expected_version, sys.modules[module].__version__), \"#f44336\")\n",
    "            else:\n",
    "                printc(\"[âœ“] {} version {} is correct.\".format(\n",
    "                    module, expected_version), \"#4caf50\")\n",
    "        except:\n",
    "            printc(\"[â€“] {} is not imported, skipping version check.\".format(\n",
    "                module), \"#03a9f4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRrGNFqRddUK"
   },
   "source": [
    "---\n",
    "## Your Implementation\n",
    "Implement your localization algorithm in the function `duet_source_separation(mic_data_folder, NUM_SOURCES)`. Do **NOT** change its function signature. You are, however, free to define and use helper functions. \n",
    "\n",
    "Your implementation for `duet_source_separation` function should **NOT** output any plots or data. It should only return the user's calculated location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UisT3IqdddUL"
   },
   "outputs": [],
   "source": [
    "def duet_source_separation(mic_data_folder, NUM_SOURCES):\n",
    "    \"\"\"DUET source separation algorithm. Write your code here.\n",
    "\n",
    "    Args:\n",
    "        mic_data_folder: name of folder (without a trailing slash) containing \n",
    "                         two mic datafiles `0.wav` and `1.wav`.\n",
    "\n",
    "    Returns:\n",
    "        NUM_SOURCES * recording_length numpy array, where NUM_SOURCES is the number of sources,\n",
    "        and recording_length is the original length of the recording (in number of samples)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Take STFT of both mics bin by bin \n",
    "\n",
    "    # constants\n",
    "    stft_seg_length = 512\n",
    "    overlap = 100\n",
    "    eps = 2.2204e-16\n",
    "\n",
    "    # Reading .wav files\n",
    "    sample_rate0, samples0 = wavfile.read(mic_data_folder+'/0.wav')\n",
    "    sample_rate1, samples1 = wavfile.read(mic_data_folder+'/1.wav')\n",
    "\n",
    "    # Correct Spectrogram\n",
    "    f0, t0, tf0 = signal.stft(samples0, fs=sample_rate0, nperseg=stft_seg_length, noverlap=overlap)\n",
    "    f1, t1, tf1 = signal.stft(samples1, fs=sample_rate1, nperseg=stft_seg_length, noverlap=overlap)\n",
    "\n",
    "    # Creating phase difference list\n",
    "    delta_phi = []\n",
    "\n",
    "    # Time frequency matrix\n",
    "    tfmat = []\n",
    "\n",
    "\n",
    "    for  j in range(0,t0.shape[0]): \n",
    "        for i in range(1,f0.shape[0]//2):\n",
    "            if np.abs(tf0[i][j]) >  .1 and np.abs(tf1[i][j]) > .1:\n",
    "                temp = tf1[i][j]/tf0[i][j]\n",
    "                delta_phi.append(np.arctan2(temp.imag, temp.real)/(f0[i]))\n",
    "                tfmat.append((i,j))\n",
    "\n",
    "    # Reshaping into column vector\n",
    "    delta_phi = np.array(delta_phi).reshape(-1,1)\n",
    "\n",
    "    # Copying for clustering reference\n",
    "    delta_phi_ref = delta_phi\n",
    "\n",
    "    # Clustering\n",
    "    codebook = cluster.vq.kmeans(obs = delta_phi_ref, k_or_guess= NUM_SOURCES)[0] # only using codebook output\n",
    "    \n",
    "    # Creating labels arrary for clustering\n",
    "    labels = np.zeros(len(tfmat))\n",
    "\n",
    "    # Filling labels\n",
    "    for i in range(len(labels)):\n",
    "        clus_diff = codebook - delta_phi[i]\n",
    "        labels[i] = np.argmin(np.abs(clus_diff))\n",
    "\n",
    "    # Smoothing\n",
    "    for i in range(len(labels-10) //10): \n",
    "        labels[i*10:i*10 + 10] = np.round(np.mean(labels[i*10:i*10 + 10]))\n",
    "\n",
    "    # Ensuring np array\n",
    "    tfmat = np.array(tfmat)\n",
    "\n",
    "    # Creating tfmat for each source\n",
    "    tfmat_persource = []\n",
    "    for i in range(NUM_SOURCES):\n",
    "        tfmat_persource.append(tfmat[labels == i])\n",
    "\n",
    "    # Setting up for inverse\n",
    "    inv = []\n",
    "\n",
    "    # Putting into each idx of inv\n",
    "    for source in tfmat_persource:\n",
    "        temp = np.zeros_like(tf0)\n",
    "        for idx in source:\n",
    "            temp[idx[0]][idx[1]] = tf0[idx[0], idx[1]]\n",
    "        inv.append(temp)\n",
    "\n",
    "    # Create output\n",
    "    output = np.zeros((NUM_SOURCES,len(samples0)))\n",
    "\n",
    "    for i in range(NUM_SOURCES):\n",
    "        output[i,:] = signal.istft(inv[i], sample_rate0, nperseg=stft_seg_length, noverlap=overlap)[1][:len(samples0)]\n",
    "\n",
    "    # to ensure .wav files are playable the output is first converted to a numpy int16 array, before being written\n",
    "    return 2**15 * output.astype('int16')\n",
    "\n",
    "\n",
    "\n",
    "    # # Resolution of frequency components per time point\n",
    "    # numfreq = len(f0)\n",
    "\n",
    "    # print(\"numfreq: \", numfreq)\n",
    "    # # Removing DC Component to prevent divide by 0 error (when dividing by k)\n",
    "    # f0 = f0[1:]\n",
    "    # tf0 = tf0[1:,:]\n",
    "    # f1 = f1[1:]\n",
    "    # tf1 = tf1[1:,:]\n",
    "\n",
    "    # # Element wise division to obtain phase difference of TF data\n",
    "    # # tfmat = np.divide(tf0+eps, tf1+eps)\n",
    "    # tfmat = (tf0+eps) / (tf1+eps)\n",
    "\n",
    "    # # Take log to obtain -jk(phi1-phi2)\n",
    "    # tfmat_log  = np.log(tfmat)\n",
    "\n",
    "    # # Take imaginary part to obtain -k(phi1-phi2)\n",
    "    # tfmat_imag = -np.imag(tfmat_log)\n",
    "\n",
    "    # delta_phi = np.zeros(tfmat_imag.shape) # (512, 119)\n",
    "\n",
    "    # print(delta_phi.shape)\n",
    "\n",
    "    # a=np.arange(1,((numfreq/2)+1))\n",
    "    # b=np.arange((-(numfreq/2)+1),-1)\n",
    "    # freq=(np.concatenate((a,b)))*((2*np.pi)/numfreq) #freq looks like saw signal\n",
    "\n",
    "    # # Divide by k (our fourier frequency indicies) to obtain (phi1-phi2)\n",
    "\n",
    "    # print(\"tfmat_imag size: \", tfmat_imag.shape)\n",
    "\n",
    "    # for i in range(tfmat_imag.shape[1]):\n",
    "    #     delta_phi[:,i] = np.divide(tfmat_imag[:,i], freq)\n",
    "    \n",
    "    # plt.pcolormesh(t1, f1, np.log(np.abs(delta_phi)))\n",
    "    # plt.ylabel('Frequency [Hz]')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # TDoA, labels = scipy.cluster.vq.kmeans2(delta_phi.flatten(), NUM_SOURCES)\n",
    "\n",
    "    # # print(\"TDoA: \", TDoA)\n",
    "    # # print(labels.shape)\n",
    "    # # print(\"Labels: \", labels[50:110])\n",
    "\n",
    "    # # We reshaped delta_phi in row_major so now we are reshaping labels with row_major to ensure proper alignment\n",
    "    # labels = labels.reshape(delta_phi.shape)\n",
    "\n",
    "    # # List of delta_phi's seperated out by their label into respective index of the list\n",
    "    # delta_phi_seperate = []\n",
    "\n",
    "    # TF_seperate = []\n",
    "\n",
    "    # for i in range(NUM_SOURCES):\n",
    "    #     # # Deep copy of our delta phi array\n",
    "    #     # temp_delta_phi = np.array(delta_phi)\n",
    "\n",
    "    #     # # Zeroing out any data not corresponding to this label\n",
    "    #     # temp_delta_phi[labels != i] = 0\n",
    "\n",
    "    #     # # Add to seperated list of delta phi's\n",
    "    #     # delta_phi_seperate.append(temp_delta_phi)\n",
    "\n",
    "    #     # testing\n",
    "    #     temp_tfmat = np.array(tf0)\n",
    "    #     temp_tfmat[labels != i] = 0\n",
    "    #     TF_seperate.append(temp_tfmat)\n",
    "\n",
    "    # plt.pcolormesh(t1, f1, np.log(np.abs(TF_seperate[2])))\n",
    "    # plt.ylabel('Frequency [Hz]')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "\n",
    "    # # we tweaking?\n",
    "    # # print(\"list info: \", delta_phi_seperate[0][1,50:55]) \n",
    "    # # print(\"truth: \", labels[1,50:55])\n",
    "        \n",
    "    # audio_seperated = []\n",
    "\n",
    "    # for i in range(NUM_SOURCES):\n",
    "    #     times, temp_audio = signal.istft(TF_seperate[i])\n",
    "    #     audio_seperated.append(temp_audio)\n",
    "\n",
    "    #     output = np.int16(temp_audio)\n",
    "    #     scipy.io.wavfile.write(\"temp\"+str(i)+\".wav\", sample_rate0, output)\n",
    "\n",
    "    # # #Plotting a basic histogram\n",
    "    # # tf_bins = len(tf0) * len(tf1)\n",
    "    # # plt.hist(delta_phi.flatten(), bins=tf_bins, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # # # Adding labels and title\n",
    "    # # plt.xlabel('Values')\n",
    "    # # plt.ylabel('Frequency')\n",
    "    # # plt.title('Basic Histogram')\n",
    "    \n",
    "    # # # Display the plot\n",
    "    # # plt.show()\n",
    "\n",
    "    # # to ensure .wav files are playable the output is first converted to a numpy int16 array, before being written to a .wav file\n",
    "    # # output = np.int16(output)\n",
    "    # # scipy.io.wavfile.write(\"temp.wav\", 22050, output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duet_source_separation_cho(mic_data_folder, NUM_SOURCES):\n",
    "    \"\"\"DUET source separation algorithm. Write your code here.\n",
    "\n",
    "    Args:\n",
    "        mic_data_folder: name of folder (without a trailing slash) containing \n",
    "                         two mic datafiles `0.wav` and `1.wav`.\n",
    "\n",
    "    Returns:\n",
    "        NUM_SOURCES * recording_length numpy array, where NUM_SOURCES is the number of sources,\n",
    "        and recording_length is the original length of the recording (in number of samples)\n",
    "\n",
    "    \"\"\"\n",
    "    fs1, x1 = scipy.io.wavfile.read(mic_data_folder + '/0.wav')\n",
    "    fs2, x2 = scipy.io.wavfile.read(mic_data_folder+ '/1.wav')\n",
    "\n",
    "    nper = 512\n",
    "    nover = 100\n",
    "    toReturn = np.zeros((NUM_SOURCES,len(x1)))\n",
    "    freqs1, times1, spec1 = signal.stft(x1,fs1, nperseg=nper, noverlap= nover) #nperseg default 256 noverlap \n",
    "    freqs2, times2, spec2 = signal.stft(x2,fs2, nperseg= nper, noverlap= nover)\n",
    "    angs = []\n",
    "    mags = []\n",
    "    indices = [] # (frequency,time)\n",
    "\n",
    "    for  j in range(0,times1.shape[0]): \n",
    "        for i in range(1,freqs1.shape[0]//2):\n",
    "            if np.abs(spec1[i][j]) >  .1 and np.abs(spec2[i][j]) > .1:\n",
    "                temp = spec2[i][j]/spec1[i][j]\n",
    "                mags.append(np.abs(temp))\n",
    "                # angs.append(np.angle(temp))\n",
    "                angs.append(np.arctan2(temp.imag, temp.real)/(freqs1[i]))\n",
    "                indices.append((i,j))\n",
    "\n",
    "\n",
    "    angs = np.array(angs).reshape(-1,1)\n",
    "    # mags = np.array(mags).reshape(-1,1)\n",
    "\n",
    "    # toClass = np.column_stack((angs,mags))\n",
    "    toClass = angs\n",
    "\n",
    "    codebook, useless  = cluster.vq.kmeans(obs = toClass, k_or_guess= NUM_SOURCES)\n",
    "    classed = np.zeros(len(indices))\n",
    "\n",
    "    for i in range(len(classed)):\n",
    "        diffs = codebook - angs[i]\n",
    "        classed[i] = np.argmin(np.abs(diffs))\n",
    "    # classifier = KMeans(n_clusters= NUM_SOURCES)\n",
    "    # classed = classifier.fit_predict(X = toClass)\n",
    "    # classifier = GaussianMixture(n_components=NUM_SOURCES)\n",
    "    # classed = classifier.fit_predict(X = toClass)\n",
    "\n",
    "    for i in range(len(classed-10) //10): # smoothing the classifications\n",
    "        classed[i*10:i*10 + 10] = np.round(np.mean(classed[i*10:i*10 + 10]))\n",
    "\n",
    "\n",
    "    indices = np.array(indices)\n",
    "\n",
    "    #good stuff i think from here on\n",
    "    sourceindeces = []\n",
    "    for i in range(NUM_SOURCES):\n",
    "        sourceindeces.append(indices[classed == i])\n",
    "\n",
    "\n",
    "\n",
    "    toinverse = []\n",
    "    for source in sourceindeces:\n",
    "        temp = np.zeros_like(spec1)\n",
    "        for idx in source:\n",
    "            temp[idx[0]][idx[1]] = spec1[idx[0], idx[1]]\n",
    "        toinverse.append(temp)\n",
    "\n",
    "    for i in range(NUM_SOURCES):\n",
    "        toReturn[i,:] = signal.istft(toinverse[i], fs1, nperseg=nper, noverlap=nover)[1][:len(x1)]\n",
    "\n",
    "    return 2**15 * toReturn.astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duet_source_separation_v2(mic_data_folder, NUM_SOURCES):\n",
    "    \"\"\"DUET source separation algorithm. Write your code here.\n",
    "\n",
    "    Args:\n",
    "        mic_data_folder: name of folder (without a trailing slash) containing \n",
    "                         two mic datafiles `0.wav` and `1.wav`.\n",
    "\n",
    "    Returns:\n",
    "        NUM_SOURCES * recording_length numpy array, where NUM_SOURCES is the number of sources,\n",
    "        and recording_length is the original length of the recording (in number of samples)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Take STFT of both mics bin by bin \n",
    "\n",
    "    # constants\n",
    "    wlen = 1024 # window length\n",
    "    timestep = 512 # of samples between adjacent time windows\n",
    "    eps = 2.2204e-16\n",
    "\n",
    "\n",
    "    # Reading .wav files\n",
    "    sample_rate0, samples0 = wavfile.read(mic_data_folder+'/0.wav')\n",
    "    sample_rate1, samples1 = wavfile.read(mic_data_folder+'/1.wav')\n",
    "\n",
    "    # Hamming Window is used for STFT\n",
    "    awin = np.hamming(wlen)\n",
    "\n",
    "    # Normalize\n",
    "    samples0=samples0/np.iinfo(samples0.dtype).max # Dividing by maximum to normalise\n",
    "    samples1=samples1/np.iinfo(samples1.dtype).max # Dividing by maximum to normalise\n",
    "\n",
    "    # Correct Spectrogram\n",
    "    f0, t0, tf0 = signal.stft(samples0, fs=sample_rate0, window=awin, nperseg=wlen)\n",
    "    f1, t1, tf1 = signal.stft(samples1, fs=sample_rate1, window=awin, nperseg=wlen)\n",
    "\n",
    "    # Resolution of frequency components per time point\n",
    "    numfreq = len(f0)\n",
    "\n",
    "    print(\"numfreq: \", numfreq)\n",
    "    # Removing DC Component to prevent divide by 0 error (when dividing by k)\n",
    "    f0 = f0[1:]\n",
    "    tf0 = tf0[1:,:]\n",
    "    f1 = f1[1:]\n",
    "    tf1 = tf1[1:,:]\n",
    "\n",
    "    # Calculate delta phi\n",
    "    delta_phi = np.angle(tf0-tf1)\n",
    "    delta_phi = np.unwrap(delta_phi, period = 2* np.pi)\n",
    "\n",
    "    plt.pcolormesh(t1, f1, np.log(np.abs(delta_phi)))\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "    # tf_bins = len(t0) * len(f0)\n",
    "    # plt.hist(delta_phi.flatten(), bins=tf_bins, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # # Adding labels and title\n",
    "    # plt.xlabel('Values')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title('Basic Histogram')\n",
    "    \n",
    "    # # Display the plot\n",
    "    # plt.show()\n",
    "\n",
    "    TDoA, labels = scipy.cluster.vq.kmeans2(delta_phi.flatten(), NUM_SOURCES)\n",
    "\n",
    "    # We reshaped delta_phi in row_major so now we are reshaping labels with row_major to ensure proper alignment\n",
    "    labels = labels.reshape(delta_phi.shape)\n",
    "\n",
    "    # List of delta_phi's seperated out by their label into respective index of the list\n",
    "    delta_phi_seperate = []\n",
    "\n",
    "    TF_seperate = []\n",
    "\n",
    "    for i in range(NUM_SOURCES):\n",
    "        # # Deep copy of our delta phi array\n",
    "        # temp_delta_phi = np.array(delta_phi)\n",
    "\n",
    "        # # Zeroing out any data not corresponding to this label\n",
    "        # temp_delta_phi[labels != i] = 0\n",
    "\n",
    "        # # Add to seperated list of delta phi's\n",
    "        # delta_phi_seperate.append(temp_delta_phi)\n",
    "\n",
    "        # testing\n",
    "        temp_tfmat = np.array(tf0)\n",
    "        temp_tfmat[labels != i] = 0\n",
    "        TF_seperate.append(temp_tfmat)\n",
    "\n",
    "    plt.pcolormesh(t1, f1, np.log(np.abs(TF_seperate[0])))\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duet_source_separation(\"dataset3\", 3)\n",
    "\n",
    "# duet_source_separation_v2(\"dataset2\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Cell\n",
    "\n",
    "# \"\"\"\n",
    "# Simply put we are solving for s within the following:\n",
    "\n",
    "# x = As\n",
    "\n",
    "# x represents an array of the audio files we are given\n",
    "# A represents our AoA matrix, which we will solve for with the DUET algorithm\n",
    "# s represents the data of the voices after being seperated, this is what we are trying to solve for\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 1: Take STFT of both mics bin by bin \n",
    "\n",
    "# # constants\n",
    "# wlen = 1024 # window length\n",
    "# timestep = 512 # of samples between adjacent time windows\n",
    "# eps = 2.2204e-16\n",
    "\n",
    "\n",
    "# # Reading .wav files\n",
    "# sample_rate0, samples0 = wavfile.read('dataset3/0.wav')\n",
    "# sample_rate1, samples1 = wavfile.read('dataset3/1.wav')\n",
    "\n",
    "# # Hamming Window is used for STFT\n",
    "# awin = np.hamming(wlen)\n",
    "\n",
    "# # Normalize\n",
    "# samples0=samples0/np.iinfo(samples0.dtype).max # Dividing by maximum to normalise\n",
    "# samples1=samples1/np.iinfo(samples1.dtype).max # Dividing by maximum to normalise\n",
    "\n",
    "# # Correct Spectrogram\n",
    "# f0, t0, tf0 = signal.stft(samples0, fs=sample_rate0, window=awin, nperseg=wlen)\n",
    "# f1, t1, tf1 = signal.stft(samples1, fs=sample_rate1, window=awin, nperseg=wlen)\n",
    "\n",
    "# # Resolution of frequency components per time point\n",
    "# numfreq = len(f0)\n",
    "\n",
    "# print(\"numfreq: \", numfreq)\n",
    "# # Removing DC Component to prevent divide by 0 error (when dividing by k)\n",
    "# f0 = f0[1:]\n",
    "# tf0 = tf0[1:,:]\n",
    "# f1 = f1[1:]\n",
    "# tf1 = tf1[1:,:]\n",
    "\n",
    "# # fmat calc\n",
    "# #calculate pos/neg frequencies for later use in delay calc ??\n",
    "\n",
    "# a=np.arange(1,((numfreq/2)+1))\n",
    "# b=np.arange((-(numfreq/2)+1),-1)\n",
    "# freq=(np.concatenate((a,b)))*((2*pi)/numfreq) #freq looks like saw signal\n",
    "\n",
    "# a=np.ones((tf0.shape[1],freq.shape[0]))\n",
    "# freq=np.asmatrix(freq)\n",
    "# a=np.asmatrix(a)\n",
    "# for i in range(a.shape[0]):\n",
    "#     a[i]=np.multiply(a[i],freq)\n",
    "# fmat=a.transpose()\n",
    "\n",
    "\n",
    "# R21 = (tf1+eps)/(tf0+eps)\n",
    "# #2.1HERE WE ESTIMATE THE RELATIVE ATTENUATION (alpha)\n",
    "# a=np.absolute(R21) #relative attenuation between the two mixtures\n",
    "# alpha = a - 1.0 / a #'alpha' (symmetric attenuation)\n",
    "# #2.2HERE WE ESTIMATE THE RELATIVE DELAY (delta)\n",
    "# delta_phi = -(np.imag((np.log(R21)/fmat)))\n",
    "# # imaginary part, 'delta' relative delay\n",
    "\n",
    "# ############### OG\n",
    "\n",
    "# # Element wise division to obtain phase difference of TF data\n",
    "# tfmat = np.divide(tf0+eps, tf1+eps)\n",
    "\n",
    "# # Take log to obtain -jk(phi1-phi2)\n",
    "# tfmat_log  = np.log(tfmat)\n",
    "\n",
    "# # Take imaginary part to obtain -k(phi1-phi2)\n",
    "# tfmat_imag = -np.imag(tfmat_log)\n",
    "\n",
    "# delta_phi = np.zeros(tfmat_imag.shape)\n",
    "\n",
    "# print(delta_phi.shape)\n",
    "\n",
    "# # Divide by k (our fourier frequency indicies) to obtain (phi1-phi2)\n",
    "# for i in range(1,tfmat_imag.shape[0]):\n",
    "#     delta_phi[i,:] = np.divide(tfmat_imag[i,:], i)\n",
    "\n",
    "# ################# OG\n",
    "\n",
    "# # # fmat\n",
    "# # a=np.arange(1,((numfreq/2)+1))\n",
    "# # b=np.arange((-(numfreq/2)+1),0)\n",
    "# # freq=(np.concatenate((a,b)))*((2*pi)/numfreq) #freq looks like saw signal\n",
    "\n",
    "# # a=np.ones((tf0.shape[1],freq.shape[0]))\n",
    "# # freq=np.asmatrix(freq)\n",
    "# # a=np.asmatrix(a)\n",
    "# # for i in range(a.shape[0]):\n",
    "# #     a[i]=np.multiply(a[i],freq)\n",
    "# # fmat=a.transpose()\n",
    "\n",
    "# # # new delta\n",
    "# # R21 = (tf2+eps)/(tf1+eps)\n",
    "# # #2.1HERE WE ESTIMATE THE RELATIVE ATTENUATION (alpha)\n",
    "# # a=np.absolute(R21) #relative attenuation between the two mixtures\n",
    "# # alpha=a-1./a #'alpha' (symmetric attenuation)\n",
    "# # #2.2HERE WE ESTIMATE THE RELATIVE DELAY (delta)\n",
    "# # delta = -(np.imag((np.log(R21)/fmat)))\n",
    "# # # imaginary part, 'delta' relative delay\n",
    "\n",
    "# # # kmeans = KMeans(n_clusters=3)\n",
    "# # # kmeans.fit(delta_phi)\n",
    "\n",
    "# # # plt.scatter(t1, f1, c=kmeans.labels_)\n",
    "# # # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #Plotting a basic histogram\n",
    "# deltaPhi_flat = delta_phi.flatten()\n",
    "\n",
    "# TDoA = scipy.cluster.vq.kmeans(deltaPhi_flat, 3)[0]\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3)\n",
    "# kmeans.fit(np.asarray(delta_phi))\n",
    "\n",
    "# print(\"labels: \", kmeans.labels_)\n",
    "\n",
    "# print(\"TDoA: \", TDoA)\n",
    "\n",
    "# print(\"del phi flattened shape: \", deltaPhi_flat.shape)\n",
    "# # tf_bins = len(t0) * len(f0)\n",
    "# # plt.hist(deltaPhi_flat, bins=tf_bins, color='skyblue', edgecolor='black')\n",
    " \n",
    "# # # Adding labels and title\n",
    "# # plt.xlabel('Values')\n",
    "# # plt.ylabel('Frequency')\n",
    "# # plt.title('Basic Histogram')\n",
    " \n",
    "# # # Display the plot\n",
    "# # plt.show()\n",
    "\n",
    "# # plt.pcolormesh(t1, f1, np.log(np.abs(tfmat)))\n",
    "# # plt.pcolormesh(t1, f1, np.log(np.abs(tf0)))\n",
    "# plt.pcolormesh(t1, f1, np.log(np.abs(delta_phi)))\n",
    "# plt.ylabel('Frequency [Hz]')\n",
    "# plt.xlabel('Time [sec]')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # deltaPhi = spectrogram / spectrogram1\n",
    "\n",
    "# # fig = plt.figure()\n",
    "# # ax = fig.gca(projection='3d')\n",
    "\n",
    "# # ax.plot_surface(frequencies[:, None], times[None, :], 10.0*np.log10(deltaPhi))\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# # plt.pcolormesh(times, frequencies, deltaPhi)\n",
    "# # plt.imshow(spectrogram)\n",
    "# # plt.ylabel('Frequency [Hz]')\n",
    "# # plt.xlabel('Time [sec]')\n",
    "# # plt.show()\n",
    "\n",
    "# # plt.pcolormesh(times1, frequencies1, np.log(deltaPhi))\n",
    "# # #plt.imshow(spectrogram)\n",
    "# # plt.ylabel('Frequency [Hz]')\n",
    "# # plt.xlabel('Time [sec]')\n",
    "# # plt.show()\n",
    "\n",
    "# # Plotting Histogram of deltaPhi\n",
    "# # print(deltaPhi.shape)\n",
    "\n",
    "# # deltaPhi_flat = deltaPhi.flatten()\n",
    "\n",
    "# # print(np.std(deltaPhi_flat))\n",
    "\n",
    "# # print(deltaPhi_flat[:20])\n",
    "\n",
    "# # #Plotting a basic histogram\n",
    "# # tf_bins = len(times) * len(frequencies)\n",
    "# # plt.hist(deltaPhi_flat, bins=tf_bins, color='skyblue', edgecolor='black')\n",
    " \n",
    "# # # Adding labels and title\n",
    "# # plt.xlabel('Values')\n",
    "# # plt.ylabel('Frequency')\n",
    "# # plt.title('Basic Histogram')\n",
    " \n",
    "# # # Display the plot\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mysy5nucddUL"
   },
   "source": [
    "---\n",
    "## Running and Testing\n",
    "Use the cell below to run and test your code, and to get an estimate of your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CXQicdJJddUL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><h4>Dataset</h4><td><td><h4>Expected Output</h4><td><td><h4>Your Output</h4><td><td><h4>Grade</h4><td><td><h4>Points Awarded</h4><td></tr><tr><td><h4>1</h4><td><td><h4>{'hello how are you'}</h4><td><td><h4>{'hello how are you'}</h4><td><td><h4>100.00%</h4><td><td><h4>5.00 / 5.0</h4><td></tr><tr><td><h4>2</h4><td><td><h4>{'how are you', 'nice to meet you'}</h4><td><td><h4>{'how are you', 'nice to meet you'}</h4><td><td><h4>100.00%</h4><td><td><h4>5.00 / 5.0</h4><td></tr><tr><td><h4>3</h4><td><td><h4>{'good morning', 'how are you', 'nice to meet you'}</h4><td><td><h4>{'good morning', 'how are you', 'nice to meet you'}</h4><td><td><h4>100.00%</h4><td><td><h4>5.00 / 5.0</h4><td></tr><tr><td><h4><i>ðŸ‘» Hidden test 1 ðŸ‘»</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i> / 10.0</h4><td></tr><tr><td><h4><i>...</i></h4><td><td><h4><i>...</i></h4><td><td><h4><i>...</i></h4><td><td><h4><i>...</i></h4><td><td><h4><i>...</i></h4><td></tr><tr><td><h4><i>ðŸ‘» Hidden test 7 ðŸ‘»</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i></h4><td><td><h4><i>???</i> / 10.0</h4><td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_score(calculated, expected):\n",
    "    student_result = set()\n",
    "    calculated = np.array(calculated)\n",
    "    if calculated.shape[0] != len(expected):\n",
    "      return 0, {'Incorrect number of sources!'}\n",
    "    for i in range(calculated.shape[0]):\n",
    "        scipy.io.wavfile.write(\"temp.wav\",22050,calculated[i,:])\n",
    "        r = sr.Recognizer()\n",
    "        with sr.AudioFile(\"temp.wav\") as source:\n",
    "            audio = r.record(source)\n",
    "        try:\n",
    "            text = r.recognize_sphinx(audio)\n",
    "            student_result.add(text.lower())\n",
    "        except:\n",
    "            student_result.add(\"Sphinx could not understand audio\")\n",
    "    score = len(student_result.intersection(expected))/len(expected)\n",
    "    return score, student_result\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    groundtruth = [{\"hello how are you\"}, {\"nice to meet you\",\"how are you\"}, {\"how are you\",\"good morning\",\"nice to meet you\"}]\n",
    "    \n",
    "    output = [['Dataset', 'Expected Output', 'Your Output', 'Grade', 'Points Awarded']]\n",
    "    for i in range(1,4):\n",
    "        directory_name = 'dataset{}'.format(i)\n",
    "        student_output = duet_source_separation(directory_name, i)\n",
    "        result = calculate_score(student_output, groundtruth[i-1])   \n",
    "        output.append([\n",
    "            str(i),\n",
    "            str(groundtruth[i-1]), \n",
    "            str(result[1]), \n",
    "            \"{:2.2f}%\".format(result[0] * 100),\n",
    "            \"{:1.2f} / 5.0\".format(result[0] * 5),\n",
    "        ])\n",
    "\n",
    "    output.append([\n",
    "        '<i>ðŸ‘» Hidden test 1 ðŸ‘»</i>', \n",
    "        '<i>???</i>', \n",
    "        '<i>???</i>', \n",
    "        '<i>???</i>', \n",
    "        \"<i>???</i> / 10.0\"])\n",
    "    output.append([\n",
    "        '<i>...</i>', \n",
    "        '<i>...</i>', \n",
    "        '<i>...</i>', \n",
    "        '<i>...</i>', \n",
    "        \"<i>...</i>\"])\n",
    "    output.append([\n",
    "        '<i>ðŸ‘» Hidden test 7 ðŸ‘»</i>', \n",
    "        '<i>???</i>', \n",
    "        '<i>???</i>', \n",
    "        '<i>???</i>', \n",
    "        \"<i>???</i> / 10.0\"])\n",
    "    display_table(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gogDNdnbddUM"
   },
   "source": [
    "---\n",
    "## Rubric\n",
    "You will be graded on the three data points provided to you (5 points each) and seven additional data points under different settings(10 points each). We will use the same code from the **Running and Testing** section above to grade all 10 traces of data. We will run ASR on your output to see if it generates the corrected separated speech signal. Output order does not matter. Percentage of grade for each data point is based on how many sources you estimated correctly (i.e., assume there are n sources, then you will get $\\frac{1}{n} * 100\\%$ for each correctedly estimated source)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_ApDXSrddUM"
   },
   "source": [
    "---\n",
    "## Submission Guidlines\n",
    "This Jupyter notebook (`MP2.ipynb`) is the only file you need to submit on Gradescope. As mentioned earlier, you will only be graded using your implementation of the `duet_source_separation` function, which should only return the calculated **NOT** output any plots or data. \n",
    "\n",
    "**Make sure any code you added to this notebook, except for import statements, is either in a function or guarded by `__main__`(which won't be run by the autograder). Gradescope will give you immediate feedback using the provided test cases. It is your responsibility to check the output before the deadline to ensure your submission runs with the autograder.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
